# ReLU
This project is a simple Python web app that shows how the ReLU activation function works. It changes all negative input values to zero and keeps positive values the same. ReLU is commonly used in neural networks because it is fast and effective.
